# 性能监控和周期分析

**创建时间**: 2026-02-04
**目的**: 监控新闻处理周期的实际耗时

---

## 🔍 监控方法

### 方法1: Render日志实时监控

在Render Dashboard → AI Service → Logs 中搜索关键词：

#### 搜索1: "本轮采集完成，耗时"
这会显示每一轮的总耗时：

```
2026-02-04 07:00:00 - 本轮采集完成，耗时 625.3 秒
2026-02-04 07:12:30 - 本轮采集完成，耗时 587.1 秒
2026-02-04 07:25:00 - 本轮采集完成，耗时 612.8 秒
```

**分析**:
- 如果耗时 > 600秒（10分钟） → 非常严重
- 如果耗时 > 300秒（5分钟） → 需要优化
- 如果耗时 < 180秒（3分钟） → 可接受

---

#### 搜索2: "上一轮采集仍在进行中"
这会显示有多少次调度被跳过：

```
2026-02-04 07:02:00 - ⚠️  上一轮采集仍在进行中，跳过本次调度
2026-02-04 07:04:00 - ⚠️  上一轮采集仍在进行中，跳过本次调度
2026-02-04 07:06:00 - ⚠️  上一轮采集仍在进行中，跳过本次调度
```

**分析**:
- 如果每次都跳过3-5次 → 间隔太短
- 如果偶尔跳过1-2次 → 可接受
- 如果从不跳过 → 间隔合理

---

### 方法2: 分步骤耗时分析

搜索每个步骤的完成日志，计算各步骤耗时：

#### 步骤1: RSS爬取
```bash
# 搜索: "步骤 1/5 完成"
2026-02-04 07:00:00 - 步骤 1/5 完成: 爬取到 659 条新闻
```

#### 步骤2: 数据库去重
```bash
# 搜索: "步骤 2/5 完成"
2026-02-04 07:00:35 - 步骤 2/5 完成: 过滤后剩余 57 条新新闻
```
**耗时**: 35秒

#### 步骤3: 保存原始新闻
```bash
# 搜索: "步骤 3/5 完成"
2026-02-04 07:00:50 - 步骤 3/5 完成: 已保存 57 条原始新闻
```
**耗时**: 15秒

#### 步骤4: AI处理 (最慢)
```bash
# 搜索: "步骤 4/5 完成"
2026-02-04 07:10:00 - 步骤 4/5 完成: AI 处理完成，生成 54 条简报
```
**耗时**: 9分钟10秒 ⚠️ **这是瓶颈！**

#### 步骤5: 保存简报
```bash
# 搜索: "步骤 5/5 完成"
2026-02-04 07:10:20 - 步骤 5/5 完成: 成功保存 54/54 条简报
```
**耗时**: 20秒

---

## 📊 性能分析

### 当前状况（用户反馈：10多分钟）

如果一次完成确实需要10+分钟：

```
步骤1: RSS爬取            30-40秒
步骤2: 数据库去重          20-30秒
步骤3: 保存原始新闻        10-20秒
步骤4: AI处理 (57条)       500-600秒 ⚠️
步骤5: 保存简报           10-20秒
─────────────────────────────────────
总计:                     570-710秒 (9.5-11.8分钟)
```

### 竞态条件严重性

**配置**: CRAWL_INTERVAL = 120秒（2分钟）
**实际耗时**: 600秒（10分钟）

**时间线**:
```
00:00 ─── 第1轮开始
02:00 ─── 第2轮触发 → 跳过（第1轮仍在运行）
04:00 ─── 第3轮触发 → 跳过
06:00 ─── 第4轮触发 → 跳过
08:00 ─── 第5轮触发 → 跳过
10:00 ─── 第1轮完成 ✅
10:00 ─── 第6轮开始
12:00 ─── 第7轮触发 → 跳过
...
```

**结果**:
- 每2分钟触发一次，但每10分钟才完成一次
- **每次完成前会跳过4-5次调度**
- 实际有效频率 ≈ 每10分钟一次（而非2分钟）

---

## 🔧 优化建议

### 优化1: 立即调整CRAWL_INTERVAL（Critical）

**当前问题**: 间隔(2分钟) << 耗时(10分钟)

**建议**: 增加到15分钟（900秒）

**操作步骤**:
1. 访问 Render Dashboard
2. 进入 AI Service
3. 点击 "Environment" 标签
4. 添加或修改环境变量：
   ```
   CRAWL_INTERVAL=900
   ```
5. 保存（会自动重新部署）

**好处**:
- 不会再有"跳过调度"的警告
- 系统压力降低
- 更稳定运行

**缺点**:
- 新闻更新延迟从2分钟增加到15分钟
- 但实际上现在也是10分钟才更新一次

---

### 优化2: 加速AI处理步骤（重要）

**问题**: 步骤4耗时9-10分钟，占总时间的90%

**原因分析**:
```python
# 当前是串行处理
for news in news_list:
    result = self.process_news(news, ...)  # 每条约10秒

# 57条 × 10秒/条 = 570秒（9.5分钟）
```

**优化方案A: 减少处理数量**
```python
# 设置每次最多处理30条
MAX_PROCESS_COUNT = 30

if len(new_news) > MAX_PROCESS_COUNT:
    logger.info(f"新闻数量({len(new_news)})超过限制，仅处理前{MAX_PROCESS_COUNT}条")
    new_news = new_news[:MAX_PROCESS_COUNT]
```

**优化方案B: 并发处理（高级）**
```python
import concurrent.futures

def batch_process(self, news_list, ...):
    """使用线程池并发处理"""
    processed = []
    failed = []

    # 使用5个线程并发
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_news = {
            executor.submit(self.process_news, news, ...): news
            for news in news_list
        }

        for future in concurrent.futures.as_completed(future_to_news):
            try:
                result = future.result()
                if result:
                    processed.append(result)
            except Exception as e:
                news = future_to_news[future]
                failed.append(news['title'][:50])

    return processed
```

**效果对比**:
| 方案 | 处理时间 | 新闻数量 | 风险 |
|------|---------|---------|------|
| 当前（串行） | 10分钟 | 57条 | 低 |
| 方案A（限制数量） | 5分钟 | 30条 | 低 |
| 方案B（并发5线程） | 2分钟 | 57条 | 中（API限流） |

---

### 优化3: 减少RSS源数量（临时）

**当前**: 118个RSS源
**问题**: 很多源返回0条或失效

**建议**: 移除失效源，只保留高质量源

检查哪些源总是返回0条：
```bash
# 在Render Logs搜索
"返回 0 条新闻"

# 常见失效源：
- CNN RSS (SSL错误)
- Reuters RSS (401认证)
- RSSHub (超时)
```

**优化**:
1. 移除30-40个失效源
2. 爬取时间从40秒降低到20秒
3. 总耗时减少20秒

---

## 📋 立即行动清单

### 优先级1: 调整间隔（10分钟，立即执行）

1. [ ] 访问Render Dashboard
2. [ ] AI Service → Environment
3. [ ] 设置 `CRAWL_INTERVAL=900`（15分钟）
4. [ ] 保存并等待重新部署

**预期效果**: 不再有"跳过调度"警告，系统稳定运行

---

### 优先级2: 监控实际耗时（5分钟）

1. [ ] 在Render Logs搜索 "本轮采集完成，耗时"
2. [ ] 记录最近5次的耗时数据
3. [ ] 计算平均值
4. [ ] 如果平均 > 600秒，考虑进一步优化

---

### 优先级3: 分析瓶颈（10分钟）

1. [ ] 搜索 "步骤 4/5 完成"，看AI处理耗时
2. [ ] 如果 > 500秒，考虑：
   - 限制每次处理数量到30条
   - 或实现并发处理

---

## 📊 监控脚本

如果你有Shell访问权限，可以运行这个脚本分析日志：

```bash
#!/bin/bash
# 分析最近10次的处理周期

echo "=== 最近10次处理周期 ==="
echo ""

# 提取耗时数据
grep "本轮采集完成，耗时" /var/log/*.log | tail -10 | while read line; do
    # 提取时间和耗时
    timestamp=$(echo $line | awk '{print $1, $2}')
    elapsed=$(echo $line | grep -oP '\d+\.\d+ 秒' | grep -oP '\d+\.\d+')

    # 转换为分钟
    minutes=$(echo "scale=1; $elapsed / 60" | bc)

    echo "$timestamp - 耗时: ${minutes}分钟 (${elapsed}秒)"
done

echo ""
echo "=== 跳过调度统计 ==="
skip_count=$(grep "上一轮采集仍在进行中" /var/log/*.log | wc -l)
echo "总跳过次数: $skip_count"

echo ""
echo "=== AI处理耗时（最近5次）==="
# 计算步骤3到步骤4的时间差
grep "步骤 4/5 完成" /var/log/*.log | tail -5
```

---

## 🎯 预期改善

### 修改前（CRAWL_INTERVAL=120秒）
- 每次耗时：10分钟
- 跳过次数：每轮4-5次
- 实际频率：每10分钟
- 系统状态：不稳定（频繁警告）

### 修改后（CRAWL_INTERVAL=900秒）
- 每次耗时：10分钟
- 跳过次数：0次
- 实际频率：每15分钟
- 系统状态：稳定

**用户体验差异**:
- 新闻延迟从"理论2分钟"（实际10分钟）变为"明确15分钟"
- 但系统更稳定，不会因为竞态条件导致处理失败

---

**建议**: **立即将CRAWL_INTERVAL改为900秒（15分钟）**，这是最快速有效的优化方案。

之后可以考虑实现并发处理来缩短处理时间，再把间隔降低到300秒（5分钟）。
