# 新闻简报平台 - 完整工作流程分析

## 📊 流程概览

```
┌─────────────────────────────────────────────────────────────────┐
│                      新闻采集流程 (每10分钟)                      │
├─────────────────────────────────────────────────────────────────┤
│  步骤1: 爬取 (397个信息源)                                        │
│     ↓                                                           │
│  步骤2: 去重 (数据库比对)                                         │
│     ↓                                                           │
│  步骤3: 质量过滤 (规则+AI阈值)                                     │
│     ↓                                                           │
│  步骤4: AI处理 (Token优化核心)                                    │
│     ↓                                                           │
│  步骤5: 存储+呈现 (MongoDB+前端)                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 1️⃣ 新闻抓取 (Crawling)

### 信息源分布
| 来源类型 | 数量 | 抓取方式 |
|----------|------|----------|
| **Twitter/X** | 265 | RSSHub Twitter API |
| **微信公众号** | 25 | RSSHub WeChat |
| **微博大V** | 15 | RSSHub Weibo |
| **YouTube频道** | 38 | RSSHub YouTube API |
| **RSS订阅** | 54 | 直接RSS解析 |
| **总计** | **397** | |

### 抓取限制
```python
# 每源抓取数量限制
- Twitter/RSS: 每源取前10条
- YouTube: 每频道取最新视频
- 所有源统一10秒超时
```

### 抓取优化
- **多线程并发**: 使用 ThreadPoolExecutor 加速抓取
- **增量抓取**: 通过数据库比对只抓新内容
- **失败重试**: 单源失败不影响整体流程

---

## 2️⃣ 新闻筛选 (Filtering)

### 筛选流程 (三层过滤)

```
原始新闻 (假设100条)
    ↓
第一层: 去重过滤 (数据库比对)
    - 检查link是否已存在
    - 批量查询 (每批500条)
    - 结果: 约30条新新闻
    ↓
第二层: 来源特殊过滤
    - Twitter: 过滤RT转推、短内容(<20字)
    - YouTube: 过滤短视频(<60秒)
    - 结果: 约25条
    ↓
第三层: 质量评分过滤
    - 内容质量评分 (1-10分)
    - 阈值: 2分以下丢弃
    - 结果: 约20条
```

### 质量评分算法
```python
# quality_filter.py
基础分: 5分
├─ 低价值关键词 (-2分): "入门指南", "震惊", "打卡"
└─ 高价值关键词 (+3分): "发布", "推出", "开源", "突破"

阈值: 2分 (低于2分丢弃)
通过率: 通常70-80%
```

### 当前过滤规则
| 过滤项 | 规则 | 影响 |
|--------|------|------|
| Twitter RT | 删除纯转推 | 减少噪音 |
| Twitter短内容 | <20字删除 | 过滤无意义内容 |
| YouTube短视频 | <60秒删除 | 过滤Shorts |
| 质量评分 | <2分删除 | 保证内容质量 |

---

## 3️⃣ AI处理 (核心Token优化)

### 🎯 Token优化策略对比

#### 方案A: 传统方式 (已弃用)
```
新闻 → AI摘要 (1次调用) → AI分类 (1次调用)
                    ↓
            每新闻2次API调用
            Token消耗: 2x
```

#### 方案B: 合并调用 (当前使用) ✅
```
新闻 → 合并AI处理 (1次调用: 摘要+分类)
                    ↓
            每新闻1次API调用
            Token消耗: 1x (节省50%)
```

### 合并Prompt设计
```python
# COMBINED_PROMPT (settings.py)
分析新闻，生成中文摘要并分类。

新闻标题: {title}
新闻内容: {content}

输出JSON格式:
{
  "title": "中文标题（30字内）",
  "summary": "事件概述：...\n\n重要细节：\n• 细节1\n• 细节2\n\n后续影响：...",
  "category": "分类代码"
}
```

### Token优化效果
| 指标 | 优化前 | 优化后 | 节省 |
|------|--------|--------|------|
| 每新闻API调用 | 2次 | 1次 | 50% |
| 每新闻Token消耗 | ~4000 | ~2000 | 50% |
| 处理100条成本 | ~$0.8 | ~$0.4 | 50% |

### AI处理并发配置
```python
# 并发处理
max_workers = 5  # 同时处理5条新闻
每新闻超时 = 60秒

# 进度报告
每5条报告一次进度
```

---

## 4️⃣ 新闻呈现 (Storage & Display)

### 数据存储
```
MongoDB 数据库
├── raw_news (原始新闻)
│   └── 存储爬取的原始内容
├── briefs (处理后简报)
│   ├── title: 中文标题
│   ├── summary: 结构化摘要
│   ├── category: 分类
│   ├── source: 来源
│   ├── link: 原文链接
│   └── published: 发布时间
└── metadata (统计信息)
```

### 简报格式
```json
{
  "title": "OpenAI发布GPT-5",
  "summary": "事件概述：OpenAI宣布推出新一代大模型GPT-5...\n\n重要细节：\n• 参数量提升10倍\n• 支持多模态输入\n• API价格降低50%\n\n后续影响：这将加速AI应用落地...",
  "category": "ai_technology",
  "source": "OpenAI (Twitter)",
  "link": "https://...",
  "published": "2026-02-07T12:00:00"
}
```

### 实时通知 (可选)
- Redis Pub/Sub 实时推送
- 新简报发布时通知前端

---

## 5️⃣ 定时调度

```python
CRAWL_INTERVAL = 600秒 (10分钟)

调度流程:
启动 → 立即执行一次 → 每10分钟执行一次
```

---

## 📈 性能指标

### 处理能力
| 指标 | 数值 |
|------|------|
| 单次采集新闻量 | 50-100条 |
| AI处理速度 | 约5-10条/分钟 |
| 单次采集耗时 | 5-15分钟 |
| 成功率 | 通常85-95% |

### Token消耗估算
```
假设:
- 每新闻平均2000 tokens (合并prompt)
- 每天采集10轮 x 50条 = 500条
- 单价: $0.0015/1K tokens (Kimi)

每日成本: 500 × 2000 × $0.0015 / 1000 = $1.5
每月成本: 约 $45
```

---

## ⚠️ 潜在问题与建议

### 当前问题
1. **AI分类不准确**: 分类prompt较长，AI可能选错分类
2. **Token still high**: 虽然合并了，但每新闻2000 tokens仍较高
3. **内容截断**: content[:1000] 可能丢失重要信息
4. **失败率**: 5-15%的新闻处理失败

### 优化建议
1. **分类优化**: 使用更简单的分类prompt，或训练专用分类器
2. **内容智能截取**: 使用文本摘要算法先提取关键段落
3. **缓存优化**: 对相似新闻复用AI结果
4. **分级处理**: 重要新闻详细处理，普通新闻简化处理

---

## 🔧 关键代码位置

| 功能 | 文件 |
|------|------|
| 主流程 | `src/main.py` |
| AI处理 | `src/processors/cloud_ai_processor.py` |
| 质量过滤 | `src/filters/quality_filter.py` |
| 爬虫 | `src/crawlers/multi_source_crawler.py` |
| Prompt配置 | `config/settings.py` |
| 信息源配置 | `config/sources_v2.py` |
