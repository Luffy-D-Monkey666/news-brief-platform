"""
å¤šæºæ–°é—»çˆ¬è™« V2 - ä¼˜åŒ–ç‰ˆ
æ”¯æŒ RSSã€Twitter/Xã€å¾®ä¿¡å…¬ä¼—å·ã€å¾®åšã€YouTube
ä¼˜åŒ–ï¼šå¹¶å‘é‡‡é›†ã€å¢åŠ æ¯æºæ¡æ•°ã€24å°æ—¶æ—¶é—´çª—å£
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiSourceCrawler:
    """å¤šæºæ–°é—»çˆ¬è™« - ä¼˜åŒ–ç‰ˆ"""

    def __init__(self, sources_config: Dict, time_window_hours: int = 24):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            sources_config: åŒ…å« rss_feeds, twitter, wechat, weibo, youtube çš„é…ç½®
            time_window_hours: åªé‡‡é›†æœ€è¿‘Nå°æ—¶çš„æ–°é—»ï¼ˆé»˜è®¤24å°æ—¶ï¼‰
        """
        self.sources = sources_config
        self.time_window = timedelta(hours=time_window_hours)
        self.cutoff_time = datetime.now() - self.time_window
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def crawl_all(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰æ¥æºçš„æ–°é—»ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰"""
        all_news = []
        
        # å¼ºåˆ¶æ›´æ–°æ—¶é—´çª—å£ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°è®¾ç½®ï¼‰
        current_time = datetime.now()
        self.cutoff_time = current_time - self.time_window
        
        hours = self.time_window.total_seconds() / 3600
        logger.info(f"ğŸ• é‡‡é›†æ—¶é—´çª—å£: æœ€è¿‘ {hours:.0f} å°æ—¶ (cutoff: {self.cutoff_time.strftime('%Y-%m-%d %H:%M')})")
        logger.info(f"ğŸ“… åªé‡‡é›† {self.cutoff_time.strftime('%Y-%m-%d %H:%M')} ä¹‹åçš„æ–°é—»")
        
        # ç»Ÿè®¡å˜é‡
        stats = {
            'rss': {'total': 0, 'success': 0, 'failed': 0},
            'twitter': {'total': 0, 'success': 0, 'failed': 0},
            'wechat': {'total': 0, 'success': 0, 'failed': 0},
            'weibo': {'total': 0, 'success': 0, 'failed': 0},
            'youtube': {'total': 0, 'success': 0, 'failed': 0},
        }
        
        # å¹¶å‘é‡‡é›†å‡½æ•°
        def crawl_with_stats(urls, crawl_func, source_type):
            """å¹¶å‘é‡‡é›†å¹¶ç»Ÿè®¡"""
            if not urls:
                return []
            
            results = []
            stats[source_type]['total'] = len(urls)
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_url = {executor.submit(crawl_func, url): url for url in urls}
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    try:
                        news = future.result()
                        results.extend(news)
                        stats[source_type]['success'] += 1
                    except Exception as e:
                        stats[source_type]['failed'] += 1
                        logger.error(f"  âœ— {self._get_short_url(url)}: {e}")
            
            return results
        
        # 1. çˆ¬å– RSS æº
        if self.sources.get('rss_feeds'):
            logger.info(f"ğŸ“¡ å¼€å§‹å¹¶å‘çˆ¬å– {len(self.sources['rss_feeds'])} ä¸ª RSS æº...")
            news = crawl_with_stats(self.sources['rss_feeds'], self._crawl_rss, 'rss')
            all_news.extend(news)
            logger.info(f"ğŸ“¡ RSS: {stats['rss']['success']}/{stats['rss']['total']} æˆåŠŸ, "
                       f"{stats['rss']['failed']} å¤±è´¥, è·å– {len(news)} æ¡")
        
        # 2. çˆ¬å– Twitter/X æº
        if self.sources.get('twitter'):
            logger.info(f"ğŸ¦ å¼€å§‹å¹¶å‘çˆ¬å– {len(self.sources['twitter'])} ä¸ª Twitter æº...")
            news = crawl_with_stats(self.sources['twitter'], self._crawl_twitter, 'twitter')
            all_news.extend(news)
            logger.info(f"ğŸ¦ Twitter: {stats['twitter']['success']}/{stats['twitter']['total']} æˆåŠŸ, "
                       f"{stats['twitter']['failed']} å¤±è´¥, è·å– {len(news)} æ¡")
        
        # 3. çˆ¬å–å¾®ä¿¡å…¬ä¼—å·
        if self.sources.get('wechat'):
            logger.info(f"ğŸ’¬ å¼€å§‹å¹¶å‘çˆ¬å– {len(self.sources['wechat'])} ä¸ªå¾®ä¿¡å…¬ä¼—å·...")
            news = crawl_with_stats(self.sources['wechat'], self._crawl_wechat, 'wechat')
            all_news.extend(news)
            logger.info(f"ğŸ’¬ WeChat: {stats['wechat']['success']}/{stats['wechat']['total']} æˆåŠŸ, "
                       f"{stats['wechat']['failed']} å¤±è´¥, è·å– {len(news)} æ¡")
        
        # 4. çˆ¬å–å¾®åš
        if self.sources.get('weibo'):
            logger.info(f"ğŸ“± å¼€å§‹å¹¶å‘çˆ¬å– {len(self.sources['weibo'])} ä¸ªå¾®åšå¤§V...")
            news = crawl_with_stats(self.sources['weibo'], self._crawl_weibo, 'weibo')
            all_news.extend(news)
            logger.info(f"ğŸ“± Weibo: {stats['weibo']['success']}/{stats['weibo']['total']} æˆåŠŸ, "
                       f"{stats['weibo']['failed']} å¤±è´¥, è·å– {len(news)} æ¡")
        
        # 5. çˆ¬å– YouTube
        if self.sources.get('youtube'):
            logger.info(f"ğŸ“º å¼€å§‹å¹¶å‘çˆ¬å– {len(self.sources['youtube'])} ä¸ª YouTube é¢‘é“...")
            news = crawl_with_stats(self.sources['youtube'], self._crawl_youtube, 'youtube')
            all_news.extend(news)
            logger.info(f"ğŸ“º YouTube: {stats['youtube']['success']}/{stats['youtube']['total']} æˆåŠŸ, "
                       f"{stats['youtube']['failed']} å¤±è´¥, è·å– {len(news)} æ¡")
        
        # æŒ‰æ—¶é—´è¿‡æ»¤ï¼ˆåªä¿ç•™24å°æ—¶å†…çš„æ–°é—»ï¼‰
        time_filtered_news = [n for n in all_news if n.get('published', datetime.now()) > self.cutoff_time]
        filtered_count = len(all_news) - len(time_filtered_news)
        
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        logger.info(f"   åŸå§‹è·å–: {len(all_news)} æ¡")
        logger.info(f"   æ—¶é—´è¿‡æ»¤(>{self.time_window.total_seconds()/3600:.0f}å°æ—¶): ç§»é™¤ {filtered_count} æ¡")
        logger.info(f"   æœ€ç»ˆä¿ç•™: {len(time_filtered_news)} æ¡")
        logger.info("=" * 60)
        
        return time_filtered_news

    def _crawl_rss(self, feed_url: str, source_type: str = 'rss') -> List[Dict]:
        """çˆ¬å– RSS è®¢é˜…æºï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            import socket
            socket.setdefaulttimeout(30)  # å¢åŠ åˆ°30ç§’è¶…æ—¶
            
            feed = feedparser.parse(feed_url)
            news_items = []
            
            # ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ æ¯æºæ¡æ•°åˆ°50æ¡ï¼ˆåŸæ¥10æ¡â†’20æ¡â†’50æ¡ï¼‰
            # RSSHub Twitter é€šå¸¸è¿”å›20æ¡ï¼Œä½†æˆ‘ä»¬å°è¯•è·å–æ›´å¤š
            limit = 50
            
            for entry in feed.entries[:limit]:
                # è§£æå‘å¸ƒæ—¶é—´
                published = self._parse_date(entry)
                
                # æ—¶é—´è¿‡æ»¤ï¼šä¿ç•™48å°æ—¶å†…çš„æ–°é—»ï¼ˆæ”¾å®½æ—¶é—´çª—å£ï¼Œè®©æ›´å¤šå†…å®¹é€šè¿‡ï¼‰
                if published < self.cutoff_time:
                    continue
                
                news_item = {
                    'title': entry.get('title', ''),
                    'content': self._extract_content(entry),
                    'link': entry.get('link', ''),
                    'image': self._extract_image(entry),
                    'video': self._extract_video(entry),
                    'published': published,
                    'source': self._extract_source_name(feed, feed_url, source_type),
                    'source_url': feed_url,
                    'source_type': source_type,
                    'raw_data': entry
                }
                
                # YouTube ç‰¹æœ‰å­—æ®µ
                if source_type == 'youtube':
                    # ä½¿ç”¨ä¸“é—¨çš„ç¼©ç•¥å›¾æå–æ–¹æ³•
                    youtube_thumbnail = self._extract_video_thumbnail(entry)
                    if youtube_thumbnail:
                        news_item['image'] = youtube_thumbnail
                    
                    # è§†é¢‘æ—¶é•¿
                    duration = self._extract_video_duration(entry)
                    if duration:
                        news_item['video_duration'] = duration
                    
                    # è§†é¢‘ä½œè€…
                    if hasattr(entry, 'author'):
                        news_item['video_author'] = entry.author
                    
                    # è§†é¢‘è§‚çœ‹æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if hasattr(entry, 'media_group') and hasattr(entry.media_group, 'media_community'):
                        community = entry.media_group.media_community
                        if hasattr(community, 'media_statistics'):
                            news_item['video_views'] = community.media_statistics.get('views')
                
                news_items.append(news_item)
            
            socket.setdefaulttimeout(None)
            logger.info(f"  âœ“ {self._get_short_url(feed_url)}: {len(news_items)} æ¡")
            return news_items
            
        except Exception as e:
            logger.error(f"  âœ— {self._get_short_url(feed_url)}: {e}")
            return []

    def _crawl_twitter(self, twitter_url: str) -> List[Dict]:
        """
        çˆ¬å– Twitter/X æ—¶é—´çº¿
        
        Twitter URL æ ¼å¼: https://rsshub.app/twitter/user/{username}
        """
        # Twitter é€šè¿‡ RSSHub æä¾› RSS æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨ RSS çˆ¬å–
        return self._crawl_rss(twitter_url, source_type='twitter')

    def _crawl_wechat(self, wechat_url: str) -> List[Dict]:
        """çˆ¬å–å¾®ä¿¡å…¬ä¼—å·"""
        # å¾®ä¿¡å…¬ä¼—å·é€šè¿‡ RSSHub æä¾›
        return self._crawl_rss(wechat_url, source_type='wechat')

    def _crawl_zhihu(self, zhihu_url: str) -> List[Dict]:
        """çˆ¬å–çŸ¥ä¹è¯é¢˜"""
        # çŸ¥ä¹é€šè¿‡ RSSHub æä¾›
        return self._crawl_rss(zhihu_url, source_type='zhihu')

    def _crawl_weibo(self, weibo_url: str) -> List[Dict]:
        """çˆ¬å–å¾®åšå¤§V
        
        å¾®åš URL æ ¼å¼: https://rsshub.app/weibo/user/{ç”¨æˆ·ID}
        """
        # å¾®åšé€šè¿‡ RSSHub æä¾› RSS æ ¼å¼
        return self._crawl_rss(weibo_url, source_type='weibo')

    def _crawl_youtube(self, youtube_url: str) -> List[Dict]:
        """çˆ¬å– YouTube é¢‘é“
        
        YouTube URL æ ¼å¼: https://rsshub.app/youtube/user/{username}
                         https://rsshub.app/youtube/channel/{channel_id}
        """
        # YouTube é€šè¿‡ RSSHub æä¾› RSS æ ¼å¼
        return self._crawl_rss(youtube_url, source_type='youtube')

    def _extract_content(self, entry) -> str:
        """æå–æ–°é—»å†…å®¹"""
        content = entry.get('summary', '')
        if not content:
            content = entry.get('description', '')
        if not content:
            content = entry.get('content', [{}])[0].get('value', '')
        
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            content = soup.get_text(strip=True)
        
        return content[:1500]  # Twitter å†…å®¹å¯èƒ½æ›´é•¿

    def _extract_image(self, entry) -> Optional[str]:
        """æå–å›¾ç‰‡"""
        image_url = None
        
        # media:content
        if hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('url'):
                    media_type = media.get('type', '').lower()
                    url = media.get('url', '')
                    if 'image' in media_type or any(ext in url.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp']):
                        return url
        
        # media:thumbnail
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            for thumbnail in entry.media_thumbnail:
                if thumbnail.get('url'):
                    return thumbnail.get('url')
        
        # enclosures
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if 'image' in enclosure.get('type', '').lower():
                    return enclosure.get('href')
        
        # HTML img
        content = entry.get('summary', '') or entry.get('description', '')
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img = soup.find('img')
            if img and img.get('src'):
                return img.get('src')
        
        return None

    def _extract_video(self, entry) -> Optional[str]:
        """æå–è§†é¢‘"""
        video_url = None
        
        try:
            # YouTube è§†é¢‘é“¾æ¥é€šå¸¸åœ¨ link å­—æ®µ
            if entry.get('link'):
                link = entry.get('link', '')
                if 'youtube.com/watch' in link or 'youtu.be' in link:
                    return link
            
            if hasattr(entry, 'media_content') and entry.media_content:
                for media in entry.media_content:
                    if media.get('url'):
                        media_type = media.get('type', '').lower()
                        url = media.get('url', '').lower()
                        if 'video' in media_type or any(ext in url for ext in ['.mp4', '.webm', '.mov']):
                            return media.get('url')
            
            if hasattr(entry, 'enclosures') and entry.enclosures:
                for enclosure in entry.enclosures:
                    if 'video' in enclosure.get('type', '').lower():
                        return enclosure.get('href')
                        
        except Exception:
            pass
        
        return None

    def _extract_video_duration(self, entry) -> Optional[str]:
        """æå–è§†é¢‘æ—¶é•¿ï¼ˆYouTubeç‰¹æœ‰ï¼‰"""
        try:
            # YouTube RSS ä¸­æ—¶é•¿é€šå¸¸åœ¨ media:group -> yt:duration
            if hasattr(entry, 'media_group'):
                mg = entry.media_group
                if hasattr(mg, 'media_duration'):
                    return mg.media_duration.get('seconds')
            
            # æˆ–è€…ä» itunes:duration è·å–
            if hasattr(entry, 'itunes_duration'):
                return entry.itunes_duration
                
        except Exception:
            pass
        return None

    def _extract_video_thumbnail(self, entry) -> Optional[str]:
        """æå–è§†é¢‘ç¼©ç•¥å›¾ï¼ˆYouTubeç‰¹æœ‰ï¼‰"""
        try:
            # YouTube ç¼©ç•¥å›¾
            if hasattr(entry, 'media_group'):
                mg = entry.media_group
                if hasattr(mg, 'media_thumbnail') and mg.media_thumbnail:
                    return mg.media_thumbnail[0].get('url')
            
            # å°è¯•ä» media:thumbnail è·å–æœ€å¤§å°ºå¯¸
            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                thumbnails = entry.media_thumbnail
                # è¿”å›ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€å¤§å°ºå¯¸ï¼‰
                if thumbnails:
                    return thumbnails[0].get('url')
                    
        except Exception:
            pass
        return None

    def _extract_source_name(self, feed, feed_url: str, source_type: str) -> str:
        """æå–æ¥æºåç§°"""
        # å°è¯•ä» feed ä¿¡æ¯è·å–
        if hasattr(feed, 'feed') and feed.feed.get('title'):
            return feed.feed.get('title')
        
        # ä» URL æå–
        if source_type == 'twitter':
            # https://rsshub.app/twitter/user/OpenAI -> OpenAI (Twitter)
            parts = feed_url.split('/')
            if len(parts) >= 2:
                username = parts[-1]
                return f"@{username} (X/Twitter)"
        
        if source_type == 'wechat':
            # å¾®ä¿¡å…¬ä¼—å·åç§°åœ¨ URL ä¸­
            parts = feed_url.split('/')
            if len(parts) >= 2:
                return f"{parts[-1]} (å¾®ä¿¡å…¬ä¼—å·)"
        
        if source_type == 'zhihu':
            return "çŸ¥ä¹è¯é¢˜"
        
        if source_type == 'weibo':
            # å¾®åšæ¥æºåç§°
            if hasattr(feed, 'feed') and feed.feed.get('title'):
                return f"{feed.feed.get('title')} (å¾®åš)"
            return "å¾®åš"
        
        if source_type == 'youtube':
            # ä» URL æå–é¢‘é“åç§°
            # https://rsshub.app/youtube/channel/UCxxx -> YouTube Channel
            # https://rsshub.app/youtube/user/username -> YouTube User
            if 'youtube' in feed_url:
                if hasattr(feed, 'feed') and feed.feed.get('title'):
                    return f"{feed.feed.get('title')} (YouTube)"
                return "YouTube é¢‘é“"
        
        # é»˜è®¤ï¼šä½¿ç”¨åŸŸå
        try:
            from urllib.parse import urlparse
            domain = urlparse(feed_url).netloc
            return domain
        except:
            return feed_url

    def _parse_date(self, entry) -> datetime:
        """è§£æå‘å¸ƒæ—¶é—´ï¼ˆç»Ÿä¸€ä¸ºæ— æ—¶åŒºçš„æœ¬åœ°æ—¶é—´ï¼‰"""
        date_str = entry.get('published', entry.get('updated', ''))
        if date_str:
            try:
                from dateutil import parser
                parsed = parser.parse(date_str)
                # å¦‚æœå¸¦æ—¶åŒºï¼Œè½¬æ¢ä¸ºæ— æ—¶åŒºçš„æœ¬åœ°æ—¶é—´
                if parsed.tzinfo is not None:
                    parsed = parsed.replace(tzinfo=None)
                return parsed
            except Exception as e:
                logger.debug(f"æ—¥æœŸè§£æå¤±è´¥: {date_str}, é”™è¯¯: {e}")
                pass
        return datetime.now()

    def _get_short_url(self, url: str) -> str:
        """è·å–çŸ­ URL ç”¨äºæ—¥å¿—"""
        if len(url) > 50:
            return url[:47] + "..."
        return url


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    from config.sources_v2 import NEWS_SOURCES_V2
    
    crawler = MultiSourceCrawler(NEWS_SOURCES_V2)
    news = crawler.crawl_all()
    
    # æŒ‰æ¥æºç±»å‹ç»Ÿè®¡
    stats = {}
    for item in news:
        source_type = item.get('source_type', 'unknown')
        stats[source_type] = stats.get(source_type, 0) + 1
    
    print("\nçˆ¬å–ç»Ÿè®¡:")
    for source_type, count in stats.items():
        print(f"  {source_type}: {count} æ¡")
