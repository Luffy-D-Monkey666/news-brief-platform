"""
æ–°é—»ç®€æŠ¥AIæœåŠ¡ V2 - å¤šæºæ”¯æŒ
æ”¯æŒ RSSã€Twitter/Xã€å¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ã€å³åˆ»
"""

import sys
import os
import time
import schedule
import logging
from datetime import datetime
from typing import Dict
from threading import Lock, Thread
from flask import Flask, jsonify

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ V2 é…ç½®
from config.sources_v2 import NEWS_SOURCES_V2
from config.settings import (
    MONGODB_URI, CRAWL_INTERVAL,
    SUMMARIZE_PROMPT, CLASSIFY_PROMPT, COMBINED_PROMPT, REDIS_URL
)
from crawlers.multi_source_crawler import MultiSourceCrawler
from processors.cloud_ai_processor import NewsProcessor
from models.database import NewsDatabase
from filters.quality_filter import ContentQualityFilter
import redis

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsServiceV2:
    """æ–°é—»æœåŠ¡ä¸»ç±» V2"""

    def __init__(self):
        # ä½¿ç”¨å¤šæºçˆ¬è™«
        self.crawler = MultiSourceCrawler(NEWS_SOURCES_V2)
        
        # ä½¿ç”¨äº‘ç«¯AIå¤„ç†å™¨ï¼ˆKimi/DeepSeek/OpenAI/Claudeï¼‰
        ai_provider = os.getenv('AI_PROVIDER', 'kimi')
        self.processor = NewsProcessor(ai_provider)
        self.db = NewsDatabase(MONGODB_URI)

        # å†…å®¹è´¨é‡è¿‡æ»¤å™¨
        self.quality_filter = ContentQualityFilter()
        logger.info("å†…å®¹è´¨é‡è¿‡æ»¤å™¨å·²å¯ç”¨")

        # æ·»åŠ é”ï¼Œé˜²æ­¢å¹¶å‘æ‰§è¡Œ
        self._lock = Lock()
        self._is_running = False

        # Redisè¿æ¥ï¼ˆå¯é€‰ï¼Œç”¨äºå®æ—¶é€šçŸ¥ï¼‰
        try:
            self.redis_client = redis.from_url(REDIS_URL)
            self.redis_client.ping()
            self.redis_enabled = True
            logger.info("Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œå°†ç¦ç”¨å®æ—¶é€šçŸ¥åŠŸèƒ½: {str(e)}")
            self.redis_enabled = False
            self.redis_client = None

    def run_cycle(self):
        """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ–°é—»é‡‡é›†å’Œå¤„ç†å¾ªç¯"""
        if not self._lock.acquire(blocking=False):
            logger.warning("âš ï¸  ä¸Šä¸€è½®é‡‡é›†ä»åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡è°ƒåº¦")
            return

        try:
            self._is_running = True
            start_time = datetime.now()

            logger.info("=" * 60)
            logger.info("NewsHub V2 - å¼€å§‹æ–°ä¸€è½®æ–°é—»é‡‡é›†")
            logger.info(f"æ”¯æŒæº: RSS + Twitter/X + å¾®ä¿¡å…¬ä¼—å· + çŸ¥ä¹ + å³åˆ»")

            # 1. çˆ¬å–æ–°é—»ï¼ˆå¤šæºï¼‰
            logger.info("æ­¥éª¤ 1/5: å¼€å§‹çˆ¬å–æ‰€æœ‰æ–°é—»æº...")
            raw_news = self.crawler.crawl_all()
            
            # æŒ‰æ¥æºç±»å‹ç»Ÿè®¡
            source_stats = {}
            for news in raw_news:
                source_type = news.get('source_type', 'unknown')
                source_stats[source_type] = source_stats.get(source_type, 0) + 1
            
            logger.info(f"æ­¥éª¤ 1/5 å®Œæˆ: çˆ¬å–åˆ° {len(raw_news)} æ¡æ–°é—»")
            logger.info(f"  æ¥æºåˆ†å¸ƒ: {source_stats}")

            if not raw_news:
                logger.warning("æ²¡æœ‰çˆ¬å–åˆ°æ–°é—»")
                return

            # 2. è¿‡æ»¤å·²å­˜åœ¨çš„æ–°é—»
            logger.info("æ­¥éª¤ 2/5: æ‰¹é‡è¿‡æ»¤é‡å¤æ–°é—»...")
            step2_start = datetime.now()

            all_links = [news['link'] for news in raw_news]
            existing_links = set()
            BATCH_SIZE = 500

            if len(all_links) <= BATCH_SIZE:
                existing_links = self.db.check_news_exists_batch(all_links)
            else:
                batch_count = (len(all_links) + BATCH_SIZE - 1) // BATCH_SIZE
                for i in range(0, len(all_links), BATCH_SIZE):
                    batch = all_links[i:i + BATCH_SIZE]
                    batch_num = i // BATCH_SIZE + 1
                    logger.info(f"  å¤„ç†ç¬¬ {batch_num}/{batch_count} æ‰¹...")
                    existing_links.update(self.db.check_news_exists_batch(batch))

            step2_elapsed = (datetime.now() - step2_start).total_seconds()
            logger.info(f"æ‰¹é‡æŸ¥è¯¢æ€»è®¡è€—æ—¶: {step2_elapsed:.1f} ç§’")

            new_news = [news for news in raw_news if news['link'] not in existing_links]
            logger.info(f"æ­¥éª¤ 2/5 å®Œæˆ: è¿‡æ»¤åå‰©ä½™ {len(new_news)} æ¡æ–°æ–°é—»")

            if not new_news:
                logger.info("æ²¡æœ‰æ–°æ–°é—»éœ€è¦å¤„ç†")
                return

            # 2.5. å†…å®¹è´¨é‡è¿‡æ»¤
            logger.info("æ­¥éª¤ 2.5/5: å†…å®¹è´¨é‡è¿‡æ»¤...")
            before_filter = len(new_news)

            quality_filtered_news = []
            skipped_twitter_rt = 0
            skipped_twitter_short = 0
            
            for news in new_news:
                title = news.get('title', '')
                # Twitter å†…å®¹éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆRTã€@mention è¿‡æ»¤ï¼‰
                if news.get('source_type') == 'twitter':
                    # è·³è¿‡çº¯è½¬æ¨ï¼ˆRT å¼€å¤´ï¼‰
                    if title.startswith('RT @'):
                        skipped_twitter_rt += 1
                        continue
                    # è·³è¿‡ä½è´¨é‡ Twitter å†…å®¹ï¼ˆå¦‚çº¯è¡¨æƒ…ã€è¿‡çŸ­ï¼‰
                    if len(title) < 20:
                        skipped_twitter_short += 1
                        continue
                
                # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼ˆ2è€Œä¸æ˜¯3ï¼‰
                if self.quality_filter.should_process(title, 'general', threshold=2):
                    quality_filtered_news.append(news)

            after_filter = len(quality_filtered_news)
            filtered_count = before_filter - after_filter

            # è¯¦ç»†æ—¥å¿—
            logger.info(f"æ­¥éª¤ 2.5/5 å®Œæˆ:")
            logger.info(f"   åŸå§‹æ–°é—»: {before_filter} æ¡")
            if skipped_twitter_rt > 0:
                logger.info(f"   - Twitter RTè¿‡æ»¤: {skipped_twitter_rt} æ¡")
            if skipped_twitter_short > 0:
                logger.info(f"   - Twitter çŸ­å†…å®¹è¿‡æ»¤: {skipped_twitter_short} æ¡")
            logger.info(f"   è´¨é‡è¿‡æ»¤ç§»é™¤: {filtered_count - skipped_twitter_rt - skipped_twitter_short} æ¡")
            logger.info(f"   æœ€ç»ˆå‰©ä½™: {after_filter} æ¡")
            logger.info(f"   é€šè¿‡ç‡: {after_filter/before_filter*100:.1f}%")

            if not quality_filtered_news:
                logger.warning("âš ï¸ è´¨é‡è¿‡æ»¤åæ²¡æœ‰å‰©ä½™æ–°é—»ï¼Œè¯·æ£€æŸ¥è¿‡æ»¤è§„åˆ™æ˜¯å¦è¿‡äºä¸¥æ ¼")
                return

            new_news = quality_filtered_news

            # 3. ä¿å­˜åŸå§‹æ–°é—»
            logger.info("æ­¥éª¤ 3/5: ä¿å­˜åŸå§‹æ–°é—»åˆ°æ•°æ®åº“...")
            for news in new_news:
                self.db.save_raw_news(news)
            logger.info(f"æ­¥éª¤ 3/5 å®Œæˆ: å·²ä¿å­˜ {len(new_news)} æ¡åŸå§‹æ–°é—»")

            # 4. AIå¤„ç†
            logger.info(f"æ­¥éª¤ 4/5: å¼€å§‹ AI å¤„ç† ({len(new_news)} æ¡æ–°é—»)...")
            
            # æ£€æŸ¥AIé…ç½®
            ai_provider = os.getenv('AI_PROVIDER', 'kimi')
            logger.info(f"ğŸ¤– å½“å‰ AI Provider: {ai_provider}")
            
            # æ£€æŸ¥API Keyæ˜¯å¦è®¾ç½®
            api_key_var = f"{ai_provider.upper()}_API_KEY"
            if not os.getenv(api_key_var):
                logger.error(f"âŒ è­¦å‘Š: ç¯å¢ƒå˜é‡ {api_key_var} æœªè®¾ç½®!")
                logger.error(f"   AIå¤„ç†å°†æ— æ³•æ­£å¸¸å·¥ä½œï¼Œè¯·è®¾ç½®API Key")
            else:
                logger.info(f"âœ… API Key å·²è®¾ç½® ({api_key_var})")
            
            logger.info(f"ğŸ’¡ Tokenä¼˜åŒ–: ä½¿ç”¨åˆå¹¶æç¤ºè¯ï¼Œä¸€æ¬¡è°ƒç”¨å®Œæˆæ‘˜è¦+åˆ†ç±»")

            processed_news = self.processor.batch_process_combined(
                new_news,
                COMBINED_PROMPT
            )
            
            # ç»Ÿè®¡å¤„ç†ç»“æœ
            success_count = len(processed_news)
            fail_count = len(new_news) - success_count
            logger.info(f"æ­¥éª¤ 4/5 å®Œæˆ:")
            logger.info(f"   AI å¤„ç†æˆåŠŸ: {success_count}/{len(new_news)} æ¡")
            if fail_count > 0:
                logger.warning(f"   âš ï¸ å¤„ç†å¤±è´¥: {fail_count} æ¡")
            logger.info(f"   æˆåŠŸç‡: {success_count/len(new_news)*100:.1f}%")

            # 5. ä¿å­˜ç®€æŠ¥
            logger.info("æ­¥éª¤ 5/5: ä¿å­˜ç®€æŠ¥åˆ°æ•°æ®åº“...")
            saved_count = 0
            publish_count = 0
            for brief in processed_news:
                brief_id = self.db.save_brief(brief)
                if brief_id:
                    saved_count += 1
                    if self.publish_brief(brief):
                        publish_count += 1

            logger.info(f"æ­¥éª¤ 5/5 å®Œæˆ:")
            logger.info(f"   æˆåŠŸä¿å­˜: {saved_count}/{len(processed_news)} æ¡")
            if self.redis_enabled:
                logger.info(f"   å‘å¸ƒé€šçŸ¥: {publish_count} æ¡")

            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"æœ¬è½®é‡‡é›†å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
            logger.info("=" * 60)

        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.error(f"é‡‡é›†å¾ªç¯å‡ºé”™ï¼ˆè€—æ—¶ {elapsed:.1f} ç§’ï¼‰: {str(e)}", exc_info=True)

        finally:
            self._is_running = False
            self._lock.release()
            logger.debug("é‡‡é›†é”å·²é‡Šæ”¾")

    def publish_brief(self, brief: Dict) -> bool:
        """å‘å¸ƒç®€æŠ¥åˆ°Redis"""
        if not self.redis_enabled or not self.redis_client:
            return False

        try:
            if '_id' in brief:
                brief['_id'] = str(brief['_id'])
            if 'published' in brief:
                brief['published'] = brief['published'].isoformat()
            if 'created_at' in brief:
                brief['created_at'] = brief['created_at'].isoformat()

            import json
            self.redis_client.publish('news:new', json.dumps(brief, ensure_ascii=False))
            logger.debug(f"å‘å¸ƒç®€æŠ¥åˆ°Redis: [{brief['category']}] {brief['title'][:30]}")
            return True
        except Exception as e:
            logger.error(f"å‘å¸ƒåˆ°Rediså¤±è´¥: {str(e)}")
            self.redis_enabled = False
            return False


# Flaskåº”ç”¨
app = Flask(__name__)

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        "status": "running",
        "service": "news-ai-service-v2",
        "version": "2.0.0",
        "features": ["rss", "twitter", "wechat", "zhihu", "jike"],
        "timestamp": datetime.now().isoformat()
    })

@app.route('/', methods=['GET'])
def root():
    """æ ¹è·¯å¾„"""
    return jsonify({
        "service": "NewsHub AI Service V2",
        "status": "running",
        "version": "2.0.0",
        "endpoints": {
            "/health": "Health check endpoint"
        }
    })


def run_flask():
    """åœ¨åå°çº¿ç¨‹è¿è¡ŒFlask"""
    port = int(os.getenv('PORT', 10000))
    logger.info(f"å¯åŠ¨Flaskå¥åº·æ£€æŸ¥æœåŠ¡åœ¨ç«¯å£ {port}")
    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("NewsHub V2 æ–°é—»ç®€æŠ¥AIæœåŠ¡å¯åŠ¨")
    logger.info("=" * 60)
    logger.info(f"AIæä¾›å•†: {os.getenv('AI_PROVIDER', 'kimi')}")
    logger.info(f"é‡‡é›†é—´éš”: {CRAWL_INTERVAL}ç§’")
    
    # æ˜¾ç¤ºæºç»Ÿè®¡
    from config.sources_v2 import TOTAL_SOURCES
    logger.info(f"æ€»è®¡æ–°é—»æº: {TOTAL_SOURCES} ä¸ª")
    
    # å¯åŠ¨Flask
    flask_thread = Thread(target=run_flask, daemon=True)
    flask_thread.start()
    logger.info("Flaskå¥åº·æ£€æŸ¥æœåŠ¡å·²å¯åŠ¨")

    service = NewsServiceV2()

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    logger.info("æ‰§è¡Œé¦–æ¬¡é‡‡é›†...")
    service.run_cycle()

    # å®šæ—¶ä»»åŠ¡
    schedule.every(CRAWL_INTERVAL).seconds.do(service.run_cycle)

    logger.info("è¿›å…¥å®šæ—¶å¾ªç¯...")
    while True:
        schedule.run_pending()
        time.sleep(1)


if __name__ == '__main__':
    main()
